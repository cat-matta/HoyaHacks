{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Youtube Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\cathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>comments_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>486000.0</td>\n",
       "      <td>['I', 'swear,', 'whoever', 'is', 'playing', 'P...</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>['I', 'swear', ',', 'whoever', 'is', 'playing'...</td>\n",
       "      <td>['i', 'swear', ',', 'whoever', 'is', 'playing'...</td>\n",
       "      <td>['i', 'swear', 'whoever', 'is', 'playing', 'pl...</td>\n",
       "      <td>['swear', 'whoever', 'playing', 'plague', 'inc...</td>\n",
       "      <td>[('swear', 'JJ'), ('whoever', 'WP'), ('playing...</td>\n",
       "      <td>[('swear', 'a'), ('whoever', 'n'), ('playing',...</td>\n",
       "      <td>['swear', 'whoever', 'play', 'plague', 'inc', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>426000.0</td>\n",
       "      <td>['I', 'am', 'getting', 'real', 'tired', 'of', ...</td>\n",
       "      <td>I am getting real tired of being part of major...</td>\n",
       "      <td>['I', 'am', 'getting', 'real', 'tired', 'of', ...</td>\n",
       "      <td>['i', 'am', 'getting', 'real', 'tired', 'of', ...</td>\n",
       "      <td>['i', 'am', 'getting', 'real', 'tired', 'of', ...</td>\n",
       "      <td>['getting', 'real', 'tired', 'part', 'major', ...</td>\n",
       "      <td>[('getting', 'VBG'), ('real', 'JJ'), ('tired',...</td>\n",
       "      <td>[('getting', 'v'), ('real', 'a'), ('tired', 'a...</td>\n",
       "      <td>['get', 'real', 'tired', 'part', 'major', 'his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>797000.0</td>\n",
       "      <td>['This', \"channel's\", 'COVID', 'reports', 'are...</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>['This', 'channel', \"'s\", 'COVID', 'reports', ...</td>\n",
       "      <td>['this', 'channel', \"'s\", 'covid', 'reports', ...</td>\n",
       "      <td>['this', 'channel', \"'s\", 'covid', 'reports', ...</td>\n",
       "      <td>['channel', \"'s\", 'covid', 'reports', 'model',...</td>\n",
       "      <td>[('channel', 'NN'), (\"'s\", 'POS'), ('covid', '...</td>\n",
       "      <td>[('channel', 'n'), (\"'s\", 'n'), ('covid', 'n')...</td>\n",
       "      <td>['channel', \"'s\", 'covid', 'report', 'model', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>105000.0</td>\n",
       "      <td>['To', 'people', 'who', 'understand', 'science...</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>['To', 'people', 'who', 'understand', 'science...</td>\n",
       "      <td>['to', 'people', 'who', 'understand', 'science...</td>\n",
       "      <td>['to', 'people', 'who', 'understand', 'science...</td>\n",
       "      <td>['people', 'understand', 'science', 'developme...</td>\n",
       "      <td>[('people', 'NNS'), ('understand', 'VBP'), ('s...</td>\n",
       "      <td>[('people', 'n'), ('understand', 'v'), ('scien...</td>\n",
       "      <td>['people', 'understand', 'science', 'developme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>['So', 'with', 'these', 'new', 'variants', 'th...</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>['So', 'with', 'these', 'new', 'variants', 'th...</td>\n",
       "      <td>['so', 'with', 'these', 'new', 'variants', 'th...</td>\n",
       "      <td>['so', 'with', 'these', 'new', 'variants', 'th...</td>\n",
       "      <td>['new', 'variants', 'whole', 'thing', 'feels',...</td>\n",
       "      <td>[('new', 'JJ'), ('variants', 'NNS'), ('whole',...</td>\n",
       "      <td>[('new', 'a'), ('variants', 'n'), ('whole', 'a...</td>\n",
       "      <td>['new', 'variant', 'whole', 'thing', 'feel', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         link_title  channel  \\\n",
       "0           0  We Know More About Those COVID-19 Variants. It...  SciShow   \n",
       "1           1  We Know More About Those COVID-19 Variants. It...  SciShow   \n",
       "2           2  We Know More About Those COVID-19 Variants. It...  SciShow   \n",
       "3           3  We Know More About Those COVID-19 Variants. It...  SciShow   \n",
       "4           4  We Know More About Those COVID-19 Variants. It...  SciShow   \n",
       "\n",
       "    no_of_views time_uploaded  \\\n",
       "0  85,566 views  Jan 30, 2021   \n",
       "1  85,566 views  Jan 30, 2021   \n",
       "2  85,566 views  Jan 30, 2021   \n",
       "3  85,566 views  Jan 30, 2021   \n",
       "4  85,566 views  Jan 30, 2021   \n",
       "\n",
       "                                             comment   upvotes  \\\n",
       "0  I swear, whoever is playing Plague Inc needs t...  486000.0   \n",
       "1  I am getting real tired of being part of  majo...  426000.0   \n",
       "2  This channel's COVID reports are a model of ho...  797000.0   \n",
       "3  To people who understand science, the developm...  105000.0   \n",
       "4  So with these new variants this whole thing fe...  205000.0   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  ['I', 'swear,', 'whoever', 'is', 'playing', 'P...   \n",
       "1  ['I', 'am', 'getting', 'real', 'tired', 'of', ...   \n",
       "2  ['This', \"channel's\", 'COVID', 'reports', 'are...   \n",
       "3  ['To', 'people', 'who', 'understand', 'science...   \n",
       "4  ['So', 'with', 'these', 'new', 'variants', 'th...   \n",
       "\n",
       "                                        comments_str  \\\n",
       "0  I swear, whoever is playing Plague Inc needs t...   \n",
       "1  I am getting real tired of being part of major...   \n",
       "2  This channel's COVID reports are a model of ho...   \n",
       "3  To people who understand science, the developm...   \n",
       "4  So with these new variants this whole thing fe...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  ['I', 'swear', ',', 'whoever', 'is', 'playing'...   \n",
       "1  ['I', 'am', 'getting', 'real', 'tired', 'of', ...   \n",
       "2  ['This', 'channel', \"'s\", 'COVID', 'reports', ...   \n",
       "3  ['To', 'people', 'who', 'understand', 'science...   \n",
       "4  ['So', 'with', 'these', 'new', 'variants', 'th...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  ['i', 'swear', ',', 'whoever', 'is', 'playing'...   \n",
       "1  ['i', 'am', 'getting', 'real', 'tired', 'of', ...   \n",
       "2  ['this', 'channel', \"'s\", 'covid', 'reports', ...   \n",
       "3  ['to', 'people', 'who', 'understand', 'science...   \n",
       "4  ['so', 'with', 'these', 'new', 'variants', 'th...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  ['i', 'swear', 'whoever', 'is', 'playing', 'pl...   \n",
       "1  ['i', 'am', 'getting', 'real', 'tired', 'of', ...   \n",
       "2  ['this', 'channel', \"'s\", 'covid', 'reports', ...   \n",
       "3  ['to', 'people', 'who', 'understand', 'science...   \n",
       "4  ['so', 'with', 'these', 'new', 'variants', 'th...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  ['swear', 'whoever', 'playing', 'plague', 'inc...   \n",
       "1  ['getting', 'real', 'tired', 'part', 'major', ...   \n",
       "2  ['channel', \"'s\", 'covid', 'reports', 'model',...   \n",
       "3  ['people', 'understand', 'science', 'developme...   \n",
       "4  ['new', 'variants', 'whole', 'thing', 'feels',...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [('swear', 'JJ'), ('whoever', 'WP'), ('playing...   \n",
       "1  [('getting', 'VBG'), ('real', 'JJ'), ('tired',...   \n",
       "2  [('channel', 'NN'), (\"'s\", 'POS'), ('covid', '...   \n",
       "3  [('people', 'NNS'), ('understand', 'VBP'), ('s...   \n",
       "4  [('new', 'JJ'), ('variants', 'NNS'), ('whole',...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [('swear', 'a'), ('whoever', 'n'), ('playing',...   \n",
       "1  [('getting', 'v'), ('real', 'a'), ('tired', 'a...   \n",
       "2  [('channel', 'n'), (\"'s\", 'n'), ('covid', 'n')...   \n",
       "3  [('people', 'n'), ('understand', 'v'), ('scien...   \n",
       "4  [('new', 'a'), ('variants', 'n'), ('whole', 'a...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  ['swear', 'whoever', 'play', 'plague', 'inc', ...  \n",
       "1  ['get', 'real', 'tired', 'part', 'major', 'his...  \n",
       "2  ['channel', \"'s\", 'covid', 'report', 'model', ...  \n",
       "3  ['people', 'understand', 'science', 'developme...  \n",
       "4  ['new', 'variant', 'whole', 'thing', 'feel', '...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(['vader_lexicon'])\n",
    "\n",
    "with open('youtube_comments_clean.csv',encoding=\"utf8\") as file:\n",
    "    df = pd.read_csv(file)\n",
    "file.close()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['time_uploaded','no_of_views','channel','no_contract','tokenized','lower','no_punct','no_stopwords','pos_tags','wordnet_pos'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stringify the lemma list\n",
    "df['lemma_str'] = [\"\".join(map(str,l)) for l in df['lemmatized']]\n",
    "df.head()\n",
    "\n",
    "n= len(df['lemma_str'])\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-651dbfcd58d9>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lemma_str'][i]=eval(df['lemma_str'][i])\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    df['lemma_str'][i]=eval(df['lemma_str'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"\"\n",
    "for index in new:\n",
    "    word+=\" \"\n",
    "    word+=index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' vaccine test least 95 effective little adverse side effect stage 4 trust anymore trust anyone biden clinton family'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
